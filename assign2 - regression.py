# -*- coding: utf-8 -*-
"""Jeanmarcos_Perez_HW2_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-MZAkNSFnJi_hJT_-NTav2GhotwSYuQt

# HW2  Regression Fun Time!

Please run the cell below to import libraries needed for this HW. Please use the autograd numpy, otherwise you will have issues.
"""

# Commented out IPython magic to ensure Python compatibility.
import autograd.numpy as np
from autograd import grad
import matplotlib.pyplot as plt
import pandas as pd

# %matplotlib inline

"""Q1) **[10 points]**

Implement the linear regression model discussed in class below. The function (called model) takes in an array of data points, x , and an array of weights, w, and returns a vector y_predicted containing the linear combination for each of the data points. This is the linear model we have discussed in class. You can assume each data point in x only has one feature.  The length of the vector being returned should be the same as x.

"""

def model(x,w):
    #your code here
    y_predicted = w[0] + np.dot(x.T, w[1:])

    return y_predicted

"""Q2) **[10 ponts]**

Implement the least squares function discussed in class below. The function should take in an array of weights, w, an array of x's and an array of ys. It should use the model function implemented above and return a float indicating the total cost.
"""

def least_squares(w,x,y):
    #your code here
    y_predicted = model(x,w)
    cost = (1/(y.size)) * (np.sum((y_predicted - y) ** 2))

    return cost

"""Q3) **[5 point]**

This one is a freebie from HW1. Copy and paste your gradient descent function here. Specifically, the one that takes in the cost function as input and return the weight and cost history. We will be using a fixed alpha for this HW. The only difference is that this function should now also take in as input an array of x and ys, corresponding to our data. The w,x, and y are given as inputs to the cost funtion and its gradient.
"""

def gradient_descent(g,alpha,max_its,w,x,y):
    #Your code here
    gradient = grad(g)   ## This is how you use the autograd library to find the gradient of a function

    # inital weight
    weight_history = []
    w = w
    weight_history.append(w)

    # initial cost using g(w)
    cost_history = []
    c = g(w,x,y)
    cost_history.append(c)

    for i in range(max_its):

      # weight at current step
      w = w - (alpha * gradient(w,x,y))
      weight_history.append(w)

      # cost at current step
      c = g(w,x,y)
      cost_history.append(c)

    return weight_history,cost_history

"""Q4) **[1 points]**  
Run the code below to import a dataset. Then, plot a scatter plot of the data (x vs y).
"""

# import the dataset
## Carefully check the path
csvname = 'kleibers_law_data.csv'
data = np.loadtxt(csvname,delimiter=',')
x = np.log(data[:-1,:])
y = np.log(data[-1:,:])

plt.scatter(x,y)
plt.show()

"""Q5) **[10 points]**

[Part 1] Use your gradient descent function to learn a linear regression model for the x and y above using the following parameters and plot the cost_history over the 1000 iterations

g = least_squares function you implemented

w = [w_0, w_1]  , where w_0 and w_1 are random numbers between -0.1 and 0.1

max_its=1000

alpha=0.01
"""

import random

g = least_squares
w = np.random.uniform(low=-0.1, high=0.1, size=2)
x = np.array(np.log(data[:-1, :]))
max_its = 1000
alpha = 0.01

weight_history, cost_history = gradient_descent(g,alpha, max_its,w,x,y)

plt.plot(cost_history)
plt.xlabel('iteration')
plt.ylabel('cost')
plt.show()

"""Q5) [Part 2]

Use the learned weights from above (note that the "learned" weights are the ones with the lowest cost) to plot the learned line. You can use the linspace method (shown below) to generate a list of xs that you can use for plotting. You need to generate a y for each of the candidate xs using the learned weights. On the same figure, also plot the scatter plot from Q4.

"""

s = np.linspace(np.min(x),np.max(x))

min_weight = weight_history[-1]
y_predicted = min_weight[0] + min_weight[1] * s

plt.scatter(x, y)
plt.plot(s, y_predicted, color = 'red')
plt.show()

"""Q6) **[1 points]**  

Run the code below to import a dataset. Then, plot a scatter plot of the data (x vs y).
"""

# load in dataset
data = np.loadtxt('regression_outliers.csv',delimiter = ',')
x = data[:-1,:]
y = data[-1:,:]

plt.scatter(x,y)
plt.show()

"""Q7) **[10 ponts]**

 Implement the least absolute deviations function discussed in class. The function should take in an array of weights, w, an array of x's and an array of ys. It should use the model function implemented in Q1  and return a float indicating the total cost.
"""

def least_absolute_deviations(w,x,y):
    #your code here
    y_predicted = model(x,w)
    cost = (1/(y.size))* (np.sum(np.abs(y_predicted-y)))

    return cost

""" Q8) **[10 points]**

[Part 1] Use the gradient descent function twice to learn two linear models using the new x and y from Q6 using the following parameters and plot the cost_history for both runs on the same plot. Make the plot for the first run blue and the plot for the second run red.

Run 1)
g = least_squares function

w = [1.0,1.0]

max_its=100

alpha=0.1

Run 2)
g = least_absoulte_deviations

w = [1.0,1.0]

max_its=100

alpha=0.1

"""

max_its = 100
alpha = 0.1
w = np.array([1.0,1.0])

#g = least_squares function
g1 = least_squares
weight_history1, cost_history1 = gradient_descent(g1, alpha, max_its, w, x, y)

#g = least_absolute_deviations function
g2 = least_absolute_deviations
weight_history2, cost_history2 = gradient_descent(g2, alpha, max_its, w, x, y)

plt.plot(cost_history1, color='blue', label='least_squares')
plt.plot(cost_history2, color='red', label='least_absolute_deviations')
plt.xlabel('iteration')
plt.ylabel('cost')
plt.legend()
plt.show()

"""Q8) **[Part 2]**

 Use the learned weights from above to plot the two learned lines (use same colors as above). You can use the linspace method again to generate a list of xs that you can use. On the same figure, also plot the scatter plot from Q6. Which of these lines look like a better fit to you?

"""

s = np.linspace(np.min(x),np.max(x))

min_weights1 = weight_history1[-1]
y_predicted1 = min_weights1[0] + min_weights1[1] * s

min_weights2 = weight_history2[-1]
y_predicted2 = min_weights2[0] + min_weights2[1] * s


plt.scatter(x, y)
plt.plot(s, y_predicted1, color='blue', label='least_squares')
plt.plot(s, y_predicted2, color='red', label='least_absolute_deviations')
plt.legend()
plt.show()

# the learned line using least absolute deviations is a better fit since it gives
# less weight to the outlier in the data set

"""Q9) **[10 points]**

[Part 1] Implement the mean squared error (MSE) and the mean absolute deviation functions from class. The functions should take in as input an array of actual ys and an array of predicted ys and return the prediction error.
"""

def MSE(y_actual,y_pred):
    #Your code here
    sum = np.sum((y_pred - y_actual) ** 2)
    error = (1/y.size) * sum

    return error

def MAD(y_actual,y_pred):
    #Your code here
    sum = np.sum(np.abs(y_pred - y_actual))
    error = (1/y.size) * sum

    return error

"""Q9) [Part 2]
Use the functions above to report the MSE and MAD for the two models learned in Q8 [Part 1], using the x and y from Q6. You should have 4 values total, two for each model. Which model is doing better? (Note that since you are evaluating the model on the training data, this corresponds to the training error)
"""

w1 = weight_history1[-1]
w2 = weight_history2[-1]

y_predicted1 = w1[0]+ w1[1] * x
y_predicted2 = w2[0]+ w2[1] * x

print("--MSE--")
print("Least Squares: ", MSE(y, y_predicted1))
print("Least Absolute Deviations: ", MSE(y, y_predicted2))

print("--MAD--")
print("Least Squares: ", MAD(y, y_predicted1))
print("Least Absolute Deviations: ", MAD(y, y_predicted2))

# According to the MSE, the Least Squares model has lower cost. According to the MAD, the
# Least Absolute Deviations model is doing better.

"""Q10) **[6 points]**

 Implement the L1 and L2 regularizers from class. Recall the regularizers take in input the weight vector and return a score based on the L1 or L2 norm of the weights
"""

def L2_regularizer(w):
    L2 = np.sum(w ** 2)
    return L2

def L1_regularizer(w):
    L1 = np.sum(abs(w))
    return L1

"""Q11) **[12 points]**

Turn the least squares function implemented in Q2 into the Ridge (L2) and Lasso (L1) least squares (covered in class) using the functions implemented in Q10. Recall that $\lambda$ is used as a hyperparameter to specify the smoothness of the function learned (higher $\lambda$ leads to simpler and smoother functions whereas lower $\lambda$ leads to better fitting to the data. $\lambda=0$ is the same as non-regularized least-squares)
"""

def ridge(w,x,y,lmbda):
    #your code here
    cost = least_squares(w,x,y) + (lmbda * L2_regularizer(w))
    return cost

def lasso(w,x,y,lmbda):
    #your code here
    cost = least_squares(w,x,y) + (lmbda * L1_regularizer(w))
    return cost

"""## For the remaining questions we will work with `weatherHistory.csv` dataset

The file 'weatherHistory.csv'has 96,454 lines, each one corresponding to a data point. Each row (i.e., data point), has several columns. Read the data file. Note that the first line is the header describing each column.
"""

weather_data = pd.read_csv("weatherHistory.csv")
weather_data.head()

""" Use the data above to set y to be the temperatures and X to be the following columns (in order): [Apparent_Temperature, Humidity, Wind_Speed, Wind_Bearing, Visibility, Pressure] Basically, we want to see whether we can predict the temperature, using the features in X."""

y = weather_data["Temperature"]
X = weather_data[["Apparent_Temperature", "Humidity", "Wind_Speed", "Wind_Bearing", "Visibility", "Pressure"]]

"""We are now going to using a well-known ML library called sklearn. If you do not have it installed, please do so using this instruction: https://scikit-learn.org/stable/install.html

sklearn comes with many models already implemented, below we import the standard linear regression, Ridge, and Lasso models from sklearn. We also import a method that can divide our data into train/test sets. Please run the cell below.
"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split

"""This library is very easy to use. We briefly went over it in the class but please use the API and user guide  ( https://scikit-learn.org) to learn exactly how to use this library.

For instance, learning a linear regression model using sklearn can be done in two lines:

linearModel = LinearRegression()

linearModel.fit(x_train, y_train)

Use the train_test_split to divide your modified data into 80% train, 20% test.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

"""Q12) **[8 points]**

Use sklearn to train a LinearRegression model using the data above. Report the performance of the model on the test data (use sklearn's MSE implementation: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html). Note that the .predict method can be used to get the y_predictions for the test xs.
"""

from sklearn.metrics import mean_squared_error

linearModel = LinearRegression()
linearModel.fit(X_train, y_train)

pred_y = linearModel.predict(X_test)
mse = mean_squared_error(y_test, pred_y)

print("MSE of Linear Regression Model:", mse)

"""Q13) **[7 points]**

Repeat Q12 but instead of LinearRegression, use the Ridge and Lasso functions. You can keep the default alpha (note that what we called lambda in the class, the hyperparameter for regularization, is called alpha in sklearn. It is the same thing).

Print the learned parameters for the Ridge and Lasso models (using .coef_). Note that the parameters below correspond to the feature vector ( [Apparent_Temperature, Humidity, Wind_Speed, Wind_Bearing, Visibility, Pressure]), in order. I.e., the first value corresponds to "Apparent_Temperature", etc. What is the difference between the ridge and lasso parameters? Which features, if any, have been eliminated by lasso?
"""

# Ridge Function
ridge = Ridge()
ridge.fit(X_train, y_train)

pred_y = ridge.predict(X_test)
mse = mean_squared_error(y_test, pred_y)

print("MSE of Ridge Model", mse)

# Lasso Function
lasso = Lasso()
lasso.fit(X_train, y_train)

pred_y2 = lasso.predict(X_test)
mse2 = mean_squared_error(y_test, pred_y2)

print("MSE of Lasso Model", mse2)

# Learned Parameters
print("--Learned Parameters--")
print("Ridge:", ridge.coef_)
print("Lasso:", lasso.coef_)

# The lasso parameters have eliminated the second and the fifth features, which correspond
# to the Humidity and Visibility features. This means the model determined these features to
# be superflous and therefore assigned them a weight of zero.