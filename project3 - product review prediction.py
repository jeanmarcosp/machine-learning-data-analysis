# -*- coding: utf-8 -*-
"""Jeanmarcos Kai Project Part 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BMBBTULS1VdznQHlnK4vOQNJ6dOub_hm
"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import *
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, Perceptron
from sklearn.ensemble import RandomForestClassifier
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from scipy.stats import uniform, randint
from sklearn.preprocessing import label_binarize
from sklearn.svm import LinearSVC, SVC
from xgboost import XGBClassifier
import scipy
from scipy.sparse import hstack
import nltk
from numpy.lib import interp
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

train = pd.read_csv("/content/amazon_train.csv")
test = pd.read_csv("/content/amazon_test.csv")

y = train['overall']

stop = set([',', '$'])

def tokenize(words):
  return [PorterStemmer().stem(w.lower()) for w in word_tokenize(words) if w not in stop]

def no_op(word):
  return word

vectorizer = TfidfVectorizer(min_df=5, max_df=0.7, ngram_range=(1,3), tokenizer=tokenize, preprocessor=no_op)
vectorizer.fit(train['reviewText'].fillna('')+ ' ' + train['summary'].fillna(''))
X = vectorizer.transform(train['reviewText'].fillna('')+ ' ' + train['summary'].fillna(''))

X_test = vectorizer.transform(test['reviewText'].fillna('')+ ' ' + test['summary'].fillna(''))

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1)

"""# Multinomial Na√Øve Bayes"""

naive_bayes = MultinomialNB()

param_grid = {
    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],
    'fit_prior': [True, False]
}

grid_search = GridSearchCV(naive_bayes, param_grid, cv=5, scoring='f1_macro')
grid_search.fit(X, y)

best_alpha = grid_search.best_params_['alpha']
best_fit_prior = grid_search.best_params_['fit_prior']

best_params = grid_search.best_params_
print("Best Hyperparameters for MNB:", best_params)

final_naive_bayes = MultinomialNB(**best_params)
final_naive_bayes.fit(X_train, y_train)
y_pred = final_naive_bayes.predict(X_valid)

# evaluation metrics
evaluation_metrics(X_valid, y_valid, y_pred)

y_sub = final_naive_bayes.predict(X_test)
submission_df = pd.DataFrame({"pred": y_sub, "id": test["id"]})
submission_df.to_csv("Jeanmarcos-Kai-MC-NB.csv", index=False)

"""# Random Forest"""

param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
}

rf_classifier = RandomForestClassifier(random_state=42)

grid_search_rf = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='f1_macro')
grid_search_rf.fit(X, y)

best_params_rf = grid_search_rf.best_params_
print("Best Hyperparameters for Random Forest:", best_params_rf)

final_rf = RandomForestClassifier(**best_params_rf)
final_rf.fit(X_train, y_train)
y_pred = final_rf.predict(X_valid)

# evaluation metrics
evaluation_metrics(X_valid, y_valid, y_pred)

y_sub = final_rf.predict(X_test)
submission_df = pd.DataFrame({"pred": y_sub, "id": test["id"]})
submission_df.to_csv("Jeanmarcos-Kai-MC-RF.csv", index=False)

"""# Logistic Regression"""

classifier = LogisticRegression(multi_class='ovr')

param_grid = {
    "max_iter": [900, 1000],
    "C": [2.0, 2.5],
}

grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)
grid_search.fit(X, y)

best_params = grid_search.best_params_
print("Best Hyperparameters for Log Reg:", best_params)

final_lr = LogisticRegression(**best_params, multi_class='ovr')
final_lr.fit(X_train, y_train)
y_pred = final_lr.predict(X_valid)

# evaluation metrics
evaluation_metrics(X_valid, y_valid, y_pred)

y_sub = final_lr.predict(X_test)
submission_df = pd.DataFrame({"pred": y_sub, "id": test["id"]})
submission_df.to_csv("Jeanmarcos-Kai-MC-LR.csv", index=False)

"""# Multiclass ROC Function"""

def roc(model, X_test, y_test, n):

    roc_auc = dict()
    fpr = dict()
    tpr = dict()

    # binarize the output
    y_test = label_binarize(y_test, classes=range(n))
    y_pred_prob = model.predict_proba(X_test)

    for i in range(n):
        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

    # the macro-average ROC curve and ROC area
    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n)]))
    mean_tpr = np.mean([interp(all_fpr, fpr[i], tpr[i]) for i in range(n)], axis=0)
    tpr["macro"] = mean_tpr
    fpr["macro"] = all_fpr
    roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

    return roc_auc["micro"]

"""# Evaluation Metrics"""

def evaluation_metrics(X_valid, y_valid, y_pred):
  confusion = confusion_matrix(y_valid, y_pred)
  roc_auc = roc(final_lr, X_valid, y_valid, n = len(y.unique()))
  macro_f1 = f1_score(y_valid, y_pred, average='macro')
  accuracy = accuracy_score(y_valid, y_pred)

  print("Confusion Matrix:")
  print(confusion)
  print("ROC AUC:", roc_auc)
  print("Macro F1 Score:", macro_f1)
  print("Accuracy:", accuracy)