# -*- coding: utf-8 -*-
"""Jeanmarcos Perez HW3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r2LNjAhdejxwhfBo8y-s2AJRqm0pQG9q

# HW3

Please run the cell below to import libraries needed for this HW. Please use the autograd numpy, otherwise you will have issues. Please remember to always use the np library for mathematical functions (e.g., np.log, np.exp, np.sum, etc)
"""

# Commented out IPython magic to ensure Python compatibility.
import autograd.numpy as np
from autograd import grad
import matplotlib.pyplot as plt
# %matplotlib inline

"""Q1) [5 points] Implement the sigmod funcation discussed in class. The function takes a value, x, as input and returns the sigmoid function applied to the value."""

def sigmoid(x):
    #Your code here
    y = 1/(1 + np.exp(-x))
    return y

"""Q2) [20 points] Implement the logistic regression model and the cross entropy cost discussed in class. Note that the logistic regression model relies on the sigmoid function (which you have implemented above) and the linear  model (which you have implemented in HW2). You can use the linear model from HW2 for this problem. Similar to the least-squares cost from HW2, the cross entropy cost takes in as input an array of weights, w, an array of x's and an array of ys and return a float indicating the total cost."""

#Your linear model from HW2
def model(x,w):
    #Your code here
    y = w[0] + np.dot(x.T, w[1:])
    return y

#The logistic regression cross entropy cost
def cross_entropy(w,x,y):
    #Your code here
    w = np.array(w)
    x = np.array(x)
    y = np.array(y)

    y_pred = model(x, w)
    sigmoid_fun = sigmoid(y_pred)

    cross_entropy_cost = np.multiply(y, np.log(sigmoid_fun)) + np.multiply((1 - y), np.log(1 - sigmoid_fun))
    cost = -1*sum(cross_entropy_cost)/len(y)
    return cost

"""Run the code below to read a data file and plot a scatter plot of x vs y."""

csvname = '2d_classification_data_v1_entropy.csv'
data = np.loadtxt(csvname,delimiter = ',')
x = data[:-1,:]
y = data[-1:,:]

plt.scatter(x,y)

"""Q3) [10 points] Move the gradient descent function from HW2. Run the function using the cross_entropy cost and the x and y from above. The parameters should be set to: max_its=2000,w=[3.0,3.0 ], and alpha=1. Save the cost and weight history returned by the gradient descent function. Plot the cost history.

"""

##gradient descent from HW2
def gradient_descent(g,alpha,max_its,w,x,y):
    #Your code here
    gradient = grad(g)   ## This is how you use the autograd library to find the gradient of a function

    # inital weight
    weight_history = []
    w = w
    weight_history.append(w)

    # initial cost using g(w)
    cost_history = []
    c = g(w,x,y)
    cost_history.append(c)

    for i in range(max_its):

      # weight at current step
      w = w - (alpha * gradient(w,x,y))
      weight_history.append(w)

      # cost at current step
      c = g(w,x,y)
      cost_history.append(c)

    return weight_history,cost_history

##Call the gradient_descent function here
weight_history, cost_history = gradient_descent(cross_entropy, 1, 1500, np.array([3.0,3.0]), x, y[0])
cost_history = np.array(cost_history)[~np.isnan(cost_history)]

##Plot the cost history here
plt.plot(cost_history)
plt.xlabel('iteration')
plt.ylabel('cost')
plt.show()

"""Q4) [10 points] Implement a logistic regression classifier that takes in the learned weights and x as input and returns the probability of the positive class (note that this is just the output of the sigmoid applied to the linear combination of the x and w)"""

def logistic_regression(learned_w,x):
    #your code here
    x_vector = np.append([1], x)
    y_pred = np.dot(x_vector, learned_w)
    positive_class_prob = sigmoid(y_pred)
    return positive_class_prob

"""Q5) [5 points] Use the learned weights from the last gradient descent run and the logistic regression function implemented above to plot the learned curve. You can use the linspace method (shown below) to generate a list of xs that you can use. You need to generate a y for each of the candidate xs using the logistic regression function and the learned weights. On the same figure, also plot the scatter plot from Q3.

"""

s = np.linspace(np.min(x),np.max(x))

learned_w = weight_history[len(cost_history)-1]
y_array = np.array([logistic_regression(learned_w, x) for x in s])

plt.scatter(x, y)
plt.plot(s, y_array)
plt.show()

"""Q6) [5 points] Slightly modify the logistic regression model above so that it returns a 1 or 0 based on the specified threshold."""

def logistic_regression_modified(learned_w,x, threshold):
    x_vector = np.append([1], x)
    y_pred = np.dot(x_vector, learned_w)
    positive_class_prob = sigmoid(y_pred)

    class_label = 1

    if positive_class_prob < threshold:
        class_label = 0

    return class_label

"""Q7) [15 points] Write a function (called evaluate) that takes in actual and predicted ys (for a binary classification problem) and return a confusion matrix and the accuracy. Use the modified logistic regression model and the evaluate function below to report the confusion matrix and accuracy for the x and y used for our training at a threshold of 0.5.  """

def evaluate(y_actual,y_pred):

    false_positive = 0
    false_negative = 0
    true_positive = 0
    true_negative = 0

    for i in range(len(y_actual)):

        if y_actual[i] == y_pred[i]:
            if y_pred[i] == 0:
                true_negative += 1
            else:
                true_positive += 1

        else:
            if y_pred[i] == 0:
                false_negative += 1
            else:
                false_positive += 1

    accuracy = (true_positive + true_negative) / len(y_actual)

    return false_positive, false_negative, true_positive, true_negative, accuracy

threshold = 0.5

y_pred = np.array([logistic_regression_modified(learned_w, xs, threshold) for xs in x[0]])
y_actual = y[0]
false_positive, false_negative, true_positive, true_negative, accuracy = evaluate(y_actual,y_pred)

print("True Positive: ", true_positive)
print("False Negative: ", false_negative)
print("False Positive: ", false_positive)
print("True Negative: ", true_negative)
print("Accuracy: ", accuracy)

"""Q8) [20 points] Implement the perceptron cost function from the class (the softmax version). Note that the perceptron cost also uses the linear model (the model function from question 2)."""

def perceptron_cost(w,x,y):
    x = np.array(x)
    w = np.array(w)
    y_pred = model(x, w)

    exponent = -1 * np.multiply(np.array(y), y_pred)
    log = np.exp(exponent) + 1
    log_error = np.log(log)

    cost = sum(log_error)/len(y)

    return cost

"""Q9) [10 points]
* Run gradient descent function using the perceptron cost and the x and y from above and the parameters set to: max_its=2000,w=[1.0,1.0], and alpha=1.0.
* Save the cost and weight history returned by the gradient descent function.
* Plot the cost history.
* Answer: Which cost seems to do better on this dataset? Why do think that is?

"""

weight_history, cost_history = gradient_descent(perceptron_cost, 1.0, 1500, np.array([1.0,1.0]), x, y[0])

plt.plot(cost_history)
plt.xlabel('iteration')
plt.ylabel('cost')
plt.show()

# the entropy cost seems to work better on this dataset which may be due to the use of sigmoid function

"""Q10) [11 points]  The file 'heart.csv' has 304 lines, each one corresponding to a data point. Each row (i.e., data point), has several columns.

* Read the data file. (Note that the first line is the header describing each column.)
* Use the data above to set y to be the "target" and X to be the remaining columns.
* Split your data into 80% train 20% test using train_test_split.
* Use sklearn to fit a logistic regression model on your training set. Use all the default parameters. Do not evaluate at this point. (You can find out about sklearn logistic regression here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
"""

## import logistic regression and the train_test split functions from sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import pandas as pd

heart_data = pd.read_csv('heart.csv')

y = heart_data["target"]
X = heart_data[["age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal"]]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

scaler = preprocessing.StandardScaler()
scaler.fit(X_train)
X_scaled = scaler.transform(X_train)

logisticReg = LogisticRegression()
logisticReg.fit(X_scaled, y_train)

"""Q11) [5 points] Use the .predict_proba function of the logistic regression model that you have learned on your X_test. Note that the .predict_proba function returns an array of tuples where each element corresponds to the predicted probability for class 0 and 1 of the data point."""

proba = logisticReg.predict_proba(X_test)

"""Q12) [12 points]

Now you will report your results.

* Filter the predicted probabilties from the last question to an array containing only the probabilites for class 1. (I.e., you should no longer have tuples.)
* Use the roc_curve function from sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) to plot the ROC curve for your predictions using the predicted probabilities for class 1 and your y_test (use the default parameters).
* Print out the thresholds generated by the roc_curve function.
* Use the roc_auc_score function from sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) to report the AUC score.
"""

from sklearn.metrics import roc_curve, roc_auc_score

class_1 = proba[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, class_1)

plt.plot(fpr, tpr)
plt.title('ROC curve')
plt.xlabel('false positive rate (fpr)')
plt.ylabel('true positive rate (tpr)')
plt.show()

for threshold in thresholds:
  print(threshold)

print("----------------------------")

auc_score = roc_auc_score(y_test, class_1)
print("AUC score: ", auc_score)