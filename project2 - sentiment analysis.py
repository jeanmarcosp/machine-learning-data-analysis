# -*- coding: utf-8 -*-
"""Jeanmarcos Kai Project Part 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DM_gAOiTqQTKJiAjZWWLv6FJP46xEafk
"""

import numpy as np
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import Perceptron, LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, f1_score, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from pprint import pprint
from scipy.sparse import hstack

train = pd.read_csv("/content/train.csv")
test = pd.read_csv("/content/test.csv")

from google.colab import drive
drive.mount('/content/drive')

## Part 1: Binary Classification

# mapping reviews (# of stars from 1-5) into a binary high or low review
# 4 or 5 stars = high review and 3 or less stars = low review

def review_to_sentiment(score):
    if score > 3:
        return 1  #high star review
    else:
        return 0  #low star review

y = pd.DataFrame()
y = train['overall'].apply(review_to_sentiment)

# vectorizing text
vectorizer = TfidfVectorizer()
vectorizer.fit(train['reviewText'].fillna('')+ ' ' + train['summary'].fillna(''))

X = vectorizer.transform(train['reviewText'].fillna('')+ ' ' + train['summary'].fillna(''))
X_test = vectorizer.transform(test['reviewText'].fillna('')+ ' ' + test['summary'].fillna(''))

X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=1)

# naive bayes

naive_bayes = MultinomialNB()

param_grid = {
    'alpha': np.linspace(0.1, 1.0, num=10).round(decimals=1),
    'fit_prior': [True, False]
}

# cross-validation to find best hyperparameter
grid_search = GridSearchCV(naive_bayes, param_grid, cv=5, scoring='f1_macro')
grid_search.fit(X, y)

best_alpha = grid_search.best_params_['alpha']
best_fit_prior = grid_search.best_params_['fit_prior']

print("Best Alpha: ", best_alpha)
print("Best Fit Prior: ", best_fit_prior)

# fitting model using best hyperparameters
final_naive_bayes = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)
final_naive_bayes.fit(X_train, y_train)
y_pred = final_naive_bayes.predict(X_validation)

# evaluation metrics
confusion = confusion_matrix(y_validation, y_pred)
roc_auc = roc_auc_score(y_validation, y_pred)
macro_f1 = f1_score(y_validation, y_pred, average='macro')
accuracy = accuracy_score(y_validation, y_pred)

print("Confusion Matrix:")
print(confusion)
print("ROC AUC:", roc_auc)
print("Macro F1 Score:", macro_f1)
print("Accuracy:", accuracy)

y_sub = final_naive_bayes.predict(X_test)
submission_df = pd.DataFrame({"preds": y_sub, "id": test["id"]})
submission_df.to_csv("Jeanmarcos-Kai[Text-Only][NB].csv", index=False)

# perceptron

perceptron = Perceptron()

param_grid = {
    'penalty': ['l1', 'l2'],
    'alpha': [0.001, 0.005, 0.01, 0.1],
    'max_iter': [1000, 1500, 2000, 2500],
    'eta0': [0.1, 0.01]
}

# cross-validation to find best hyperparameter
grid_search = GridSearchCV(perceptron, param_grid, cv=5, scoring='f1_macro')
grid_search.fit(X, y)

best_alpha = grid_search.best_params_['alpha']
best_penalty = grid_search.best_params_['penalty']
best_max_iter = grid_search.best_params_['max_iter']
best_eta0 = grid_search.best_params_['eta0']

print("Best Alpha: ", best_alpha)
print("Best Penalty: ", best_penalty)
print("Best Max Iter: ", best_max_iter)
print("Best Eta0: ", best_eta0)

# fitting model using best hyperparameters
final_perceptron = Perceptron(penalty=best_penalty, alpha=best_alpha, max_iter=best_max_iter, eta0=best_eta0)
final_perceptron.fit(X_train, y_train)
y_pred = final_perceptron.predict(X_validation)

# evaluation metrics
confusion = confusion_matrix(y_validation, y_pred)
roc_auc = roc_auc_score(y_validation, y_pred)
macro_f1 = f1_score(y_validation, y_pred, average='macro')
accuracy = accuracy_score(y_validation, y_pred)

print("Confusion Matrix:")
print(confusion)
print("ROC AUC:", roc_auc)
print("Macro F1 Score:", macro_f1)
print("Accuracy:", accuracy)

y_sub = final_perceptron.predict(X_test)
submission_df = pd.DataFrame({"preds": y_sub, "id": test["id"]})
submission_df.to_csv("Jeanmarcos-Kai[Text-Only][Percep].csv", index=False)

# logistic regression

logreg = LogisticRegression()

param_grid = {
    "max_iter": [900, 1000],
    "solver": ['lbfgs', 'liblinear'],
    "C": [2.0, 3.5]
}

# cross-validation to find best hyperparameter
grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='f1_macro')
grid_search.fit(X, y)

best_solver = grid_search.best_params_['solver']
best_c = grid_search.best_params_['C']
best_max_iter = grid_search.best_params_['max_iter']

print("Best Solver: ", best_solver)
print("Best C: ", best_c)
print("Best Max Iter: ", best_max_iter)

# fitting model using best hyperparameters
final_logreg = LogisticRegression(C=best_c, solver=best_solver, max_iter=best_max_iter)
final_logreg.fit(X_train, y_train)
y_pred = final_logreg.predict(X_validation)

# evaluation metrics
confusion = confusion_matrix(y_validation, y_pred)
roc_auc = roc_auc_score(y_validation, y_pred)
macro_f1 = f1_score(y_validation, y_pred, average='macro')
accuracy = accuracy_score(y_validation, y_pred)

print("Confusion Matrix:")
print(confusion)
print("ROC AUC:", roc_auc)
print("Macro F1 Score:", macro_f1)
print("Accuracy:", accuracy)

y_sub = final_logreg.predict(X_test)
submission_df = pd.DataFrame({"preds": y_sub, "id": test["id"]})
submission_df.to_csv("Jeanmarcos-Kai[Text-Only][LogReg].csv", index=False)

## Part 2: Sentiment Analysis

X_sentiment_train = train['reviewText'].fillna('')+ ' ' + train['summary'].fillna('')
X_sentiment_test = test['reviewText'].fillna('')+ ' ' + test['summary'].fillna('')

#analyzing sentiment
analyzer = SentimentIntensityAnalyzer()

#sentiment of training data set
sentiment_features = []
for text in X_sentiment_train:
    sentiment_scores = analyzer.polarity_scores(text)
    sentiment_features.append(sentiment_scores)

sentiment_df = pd.DataFrame(sentiment_features)

#sentiment of testing data set
sentiment_features_test = []
for text in X_sentiment_test:
    sentiment_scores = analyzer.polarity_scores(text)
    sentiment_features_test.append(sentiment_scores)

sentiment_df_test = pd.DataFrame(sentiment_features_test)

# Subtask 1
## Predicting with a classifier using only sentiment scores, note the low macro f1 score

X_train, X_valid, y_train, y_valid = train_test_split(sentiment_df, y, test_size=0.2, random_state=42)

logreg = LogisticRegression(C=3.5, solver='lbfgs', max_iter=900)
logreg.fit(X_train, y_train)
y_pred = logreg.predict(X_valid)

confusion = confusion_matrix(y_valid, y_pred)
accuracy = accuracy_score(y_valid, y_pred)
roc_auc = roc_auc_score(y_valid, y_pred)
macro_f1 = f1_score(y_valid, y_pred)

print("Confusion Matrix:")
print(confusion)
print("ROC AUC:", roc_auc)
print("Macro F1 Score:", macro_f1)
print("Accuracy:", accuracy)

# Subtask 2
## Predicting with a classifier using sentiment scores and pure text, note the higher macro f1 score

# Using hstack to combine X which has only text, and sentiment_df which has the sentiment scores
# X_text is the same as X except it comes from test.csv instead of train.csv
X_combined = hstack([X, sentiment_df])
X_combined_test = hstack([X_test, sentiment_df_test])

X_train, X_valid, y_train, y_valid = train_test_split(X_combined, y, test_size=0.2, random_state=42)

logreg_sentiment = LogisticRegression(C=3.5, solver='lbfgs', max_iter=900)
logreg_sentiment.fit(X_train, y_train)
y_pred = logreg_sentiment.predict(X_valid)

confusion = confusion_matrix(y_valid, y_pred)
accuracy = accuracy_score(y_valid, y_pred)
roc_auc = roc_auc_score(y_valid, y_pred)
macro_f1 = f1_score(y_valid, y_pred)

print("Confusion Matrix:")
print(confusion)
print("ROC AUC:", roc_auc)
print("Macro F1 Score:", macro_f1)
print("Accuracy:", accuracy)

y_sub = logreg_sentiment.predict(X_combined_test)
submission_df = pd.DataFrame({"preds": y_sub, "id": test["id"]})
submission_df.to_csv("Jeanmarcos-Kai[Text-Sentiment][LogReg].csv", index=False)