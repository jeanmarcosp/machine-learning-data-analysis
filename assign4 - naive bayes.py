# -*- coding: utf-8 -*-
"""Jeanmarcos Perez HW4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17hXBqj6Y_8T0cyKzUFnVgPBEGVRejaIp
"""

# Commented out IPython magic to ensure Python compatibility.
import autograd.numpy as np
from autograd import grad
import matplotlib.pyplot as plt
import pandas as pd
import math
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score

cluster_data = pd.read_csv("/content/hw4_cluster.csv")
naive_data = pd.read_csv("/content/hw4_naive.csv")

## Part 1: Na√Øve Bayes Classification

y = naive_data["Label"]
X = naive_data[["Feature_1", "Feature_3", "Feature_3", "Feature_4", "Feature_5", "Feature_6"]]

y = y.to_numpy()
X = X.to_numpy()

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1)

class MultinomialNaiveBayesFromScratch:

    # constructor
    def __init__(self):
        pass

    # fit function, fits classifier to training data
    def fit(self, X, y):

        # unique class labels (0 or 1)
        self.classes = np.unique(y)

        # array to store class probabilities, initialized with zeroes
        self.class_probs = np.zeros(len(self.classes))

        # dictionary to store feature probabilities for each class
        self.feature_probs = {}

        # iterate through each class: [0,1]
        for c in self.classes:

            # filter data for the current class
            class_data = []
            for i in range(len(y)):
                if y[i] == c:
                    class_data.append(X[i])
            class_data = np.array(class_data)

            # total count of features in this class
            count = np.sum(class_data)

            # class probabilities with smoothing of 1
            self.class_probs[c] = (len(class_data) + 1) / (len(y) + 1 * len(self.classes))

            # feature probabilities with smoothing of 1
            prob = np.sum(class_data, axis=0)
            self.feature_probs[c] = (prob + 1) / (count + 1 * X.shape[1])

    # predict function
    def predict(self, X):

        # array to store predictions (0 or 1)
        predictions = []

        # iterate through each row of data in X
        for data in X:

            # array to store posterior probabilities for each class
            posteriors = []

            # iterate through each class (0 and 1) and calculate posterior probability
            for c in self.classes:

                # initialize the posterior probability with the class prior
                posterior = np.log(self.class_probs[c])

                for i, feature in enumerate(data):

                    # update the posterior with the log likelihood of the features
                    posterior += np.log(self.feature_probs[c][i]) * feature

                posteriors.append(posterior)

            # find the class with the highest posterior probability and make that the prediction
            max_index = max(range(len(posteriors)), key=lambda i: posteriors[i])
            predictions.append(max_index)

        return predictions

nb_classifier = MultinomialNaiveBayesFromScratch()
nb_classifier.fit(X_train, y_train)
y_pred = nb_classifier.predict(X_valid)

# evaluation metrics
macro_f1 = f1_score(y_valid, y_pred, average='macro')
accuracy = accuracy_score(y_valid, y_pred)

print("Macro F1 Score:", macro_f1)
print("Accuracy:", accuracy)

class GaussianNaiveBayesFromScratch:

    def __init__(self):
        self.means = {}
        self.variances = {}
        self.priors = {}

    # fit function, fits classifier to training data
    def fit(self, X, y):

        # unique class labels
        self.classes = np.unique(y)

        # iterate through each class: [0,1]
        for c in self.classes:

            # filter data for the current class
            class_data = []
            for i in range(len(y)):
                if y[i] == c:
                    class_data.append(X[i])
            class_data = np.array(class_data)

            # mean and variance for each feature using numpy functions
            self.means[c] = np.mean(class_data, axis=0)
            self.variances[c] = np.var(class_data, axis=0)

            # class priors
            self.priors[c] = len(class_data) / len(X)

    # predict function
    def predict(self, X):

        # array to store predictions (0 or 1)
        predictions = []

        # iterate through each row of data in X
        for data in X:

            # array to store posterior probabilities for each class
            posteriors = []

            # iterate through each class (0 and 1) and calculate posterior probability
            for c in self.classes:

                # log likelihood using Gaussian distribution
                log_likelihood = -0.5 * (np.sum(np.log(2 * np.pi * self.variances[c])) + np.sum(((data - self.means[c]) ** 2) / self.variances[c]))

                # calculate posterior probability by adding the prior probability to the log likelihood
                posterior = log_likelihood + np.log(self.priors[c])

                posteriors.append(posterior)
                # print(np.argmax(posteriors))

            # find the class with the highest posterior probability and make that the prediction
            max_index = max(range(len(posteriors)), key=lambda i: posteriors[i])
            predictions.append(max_index)

        return predictions

gnb_classifier = GaussianNaiveBayesFromScratch()
gnb_classifier.fit(X_train, y_train)
y_pred = gnb_classifier.predict(X_valid)

# evaluation metrics
macro_f1 = f1_score(y_valid, y_pred, average='macro')
accuracy = accuracy_score(y_valid, y_pred)

print("Macro F1 Score:", macro_f1)
print("Accuracy:", accuracy)

## Part 2: Clustering

data_points = tuple(zip(cluster_data['x1'], cluster_data['x2']))

def euclidean_distance(data1, data2):
    x1 = data1[0]
    y1 = data1[1]

    x2 = data2[0]
    y2 = data2[1]

    return math.sqrt((x2-x1) ** 2 + (y2-y1) ** 2)

def mean_funtion(cluster):
    x = 0
    y = 0

    for point in cluster:
        x += point[0]
        y += point[1]

    average_x = x / len(cluster)
    average_y = y / len(cluster)

    return average_x, average_y

def kmeans(data_points, max_iter, k, init_method, centroid_function):

    # initializing k clusters
    clusters = [[] for i in range(k)]

    # setting initialization method
    if init_method == "random_split":
        centroids = [data_points[i] for i in range(k)]

    elif init_method == "random_seed_selection":
        centroids = [data_points[np.random.randint(len(data_points))] for _ in range(k)]

    for i in range(max_iter):
        for each_data_point in data_points:
            min_idx = 0
            min_distance = float('inf')

            for j in range(len(centroids)):
                centroid = centroids[j]
                curr_distance = euclidean_distance(centroid, each_data_point)

                if curr_distance <= min_distance:
                    min_distance = curr_distance
                    min_idx = j

            clusters[min_idx].append(each_data_point)

        # no change to cluster at last step = stop
        new_centroids = []
        for m in range(len(clusters)):
            new_centroids.append(centroid_function(clusters[m]))

        if new_centroids == centroids:
            break
        else:
            centroids = new_centroids

        # reach max iterations = stop
        if i == max_iter - 1:
            break
        else:
            clusters = [[] for i in range(k)]

    return clusters

def silhouette_score(clusters):
    a_values = []
    b_values = []

    # iterate through all the clusters
    for cluster1 in clusters:

        for i, point1 in enumerate(cluster1):

            distance_ax = 0
            count_ax = 0

            # average distance (a) of point1 to other points in the same cluster
            for point2 in cluster1:

                if point1 is not point2:
                    distance_ax += euclidean_distance(point1, point2)
                    count_ax += 1

            # average
            ax = distance_ax / count_ax
            a_values.append(ax)

            # minimum average distance (b) of point1 to points in other clusters
            min_bx = float('inf')
            for cluster2 in clusters:

                # make sure we are not comparing same cluster
                if cluster2 != cluster1:

                    distance_bx = 0
                    count_bx = 0

                    # average distance of point1 to points in cluster2
                    for point2 in cluster2:
                        distance_bx += euclidean_distance(point1, point2)
                        count_bx += 1

                    bx = distance_bx / count_bx
                    min_bx = min(min_bx, bx)

            b_values.append(min_bx)


    # calculate the silhouette scores for each point and store them in a list
    silhouette_scores = []
    for i in range(len(a_values)):
      a = a_values[i]
      b = b_values[i]
      silhouette_score = (b - a) / max(a, b)
      silhouette_scores.append(silhouette_score)

    return np.mean(silhouette_scores)

clusters3 = kmeans(data_points, 50, 5, "random_seed_selection", mean_funtion)
silhouette = silhouette_score(clusters3)
print("silhouette score:", silhouette)

## Extra Credit

max_iter = 100
init_method = "random_split"
centroid_fun = mean_funtion

for k in range(2, 6):
    clusters= kmeans(data_points, max_iter, k, init_method, centroid_fun)
    sil = silhouette_score(clusters)
    print("k = {i}: silhouette score = {score}".format(i=k, score=sil))

# k = 2 seems to be the best value of k since it returns the highest silhouette score